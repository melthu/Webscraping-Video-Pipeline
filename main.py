"""
Main script for running the video pipeline.
"""

import os
import sys
import logging
import argparse
import json
from typing import Dict, Any, List

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import components
from scrapers.pexels_scraper import PexelsScraper
from scrapers.videvo_scraper import VidevoScraper
from scrapers.nasa_scraper import NASAScraper
from scrapers.internet_archive_scraper import InternetArchiveScraper
from scrapers.wikimedia_scraper import WikimediaScraper
from scrapers.coverr_scraper import CoverrScraper
from scrapers.noaa_scraper import NOAAScraper

from validators.validation_pipeline import ValidationPipeline
from storage.cloud_storage import CloudStorageUploader
from processors.batch_processor import BatchProcessor

from dotenv import load_dotenv
load_dotenv()

def setup_logging(log_dir: str = "logs", log_level: str = "INFO") -> None:
    """
    Set up logging configuration.
    
    Args:
        log_dir: Directory for log files
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    """
    # Create log directory if it doesn't exist
    os.makedirs(log_dir, exist_ok=True)
    
    # Set up logging level
    numeric_level = getattr(logging, log_level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f"Invalid log level: {log_level}")
    
    # Configure logging
    logging.basicConfig(
        level=numeric_level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(log_dir, "pipeline.log")),
            logging.StreamHandler()
        ]
    )

def load_config(config_path: str) -> Dict[str, Any]:
    """
    Load configuration from JSON file.
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        Configuration dictionary
    """
    try:
        with open(config_path, "r") as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"Error loading configuration: {str(e)}")
        # Return default configuration
        return {
            "scrapers": {
                "pexels": {"per_page": 20},
                "videvo": {"per_page": 20, "request_delay": 1.0},
                "nasa": {"per_page": 20, "request_delay": 3.6},
                "internet_archive": {"per_page": 20, "request_delay": 1.0},
                "wikimedia": {"per_page": 20, "request_delay": 1.0},
                "coverr": {"per_page": 20, "request_delay": 3.0},
                "noaa": {"per_page": 20, "request_delay": 2.0}
            },
            "validators": {
                "text_detection": {
                    "sampling_rate": 30,
                    "confidence_threshold": 70,
                    "min_text_detections": 3
                },
                "cut_scene": {
                    "threshold": 0.35,
                    "min_scene_changes": 2,
                    "frame_skip": 1
                },
                "resolution": {
                    "min_width": 512,
                    "min_height": 512
                },
                "ai_content": {
                    "ai_keywords": [
                        "ai generated", "artificial intelligence", "generated by ai", 
                        "ai created", "stable diffusion", "midjourney", "dall-e", 
                        "generative ai", "synthetic", "deepfake", "gan", "neural network",
                        "machine learning", "ml generated", "computer generated"
                    ]
                },
                "physics": {
                    "sampling_rate": 15,
                    "optical_flow_threshold": 50.0,
                    "acceleration_threshold": 100.0,
                    "min_violations": 3
                },
                "log_file": "logs/validation.log",
                "detailed_logs": True
            },
            "storage": {
                "provider": "aws",
                "bucket_name": "video-pipeline-bucket",
                "folder_prefix": "videos/",
                "region": "us-east-1",
                "upload_history_file": "logs/upload_history.json",
                "max_retries": 3,
                "retry_delay": 2
            },
            "batch": {
                "download_dir": "downloads",
                "processed_dir": "processed",
                "failed_dir": "failed",
                "batch_size": 10,
                "max_workers": 4,
                "disk_space_threshold": 1073741824,  # 1 GB
                "state_file": "logs/batch_state.json"
            }
        }

def initialize_pipeline(config: Dict[str, Any]) -> BatchProcessor:
    """
    Initialize the video pipeline with all components.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Initialized batch processor
    """
    # Initialize validation pipeline
    validation_pipeline = ValidationPipeline(config["validators"])
    
    # Initialize cloud storage uploader
    cloud_uploader = CloudStorageUploader(config["storage"])
    
    # Initialize batch processor
    batch_processor = BatchProcessor(config["batch"])
    
    # Initialize scrapers
    scrapers = {
        "pexels": PexelsScraper(config["scrapers"]["pexels"]),
        "videvo": VidevoScraper(config["scrapers"]["videvo"]),
        "nasa": NASAScraper(config["scrapers"]["nasa"]),
        "internet_archive": InternetArchiveScraper(config["scrapers"]["internet_archive"]),
        "wikimedia": WikimediaScraper(config["scrapers"]["wikimedia"]),
        "coverr": CoverrScraper(config["scrapers"]["coverr"]),
        "noaa": NOAAScraper(config["scrapers"]["noaa"])
    }
    
    # Register scrapers with batch processor
    for name, scraper in scrapers.items():
        batch_processor.register_scraper(name, scraper)
    
    # Set validation pipeline and cloud uploader
    batch_processor.set_validation_pipeline(validation_pipeline)
    batch_processor.set_cloud_uploader(cloud_uploader)
    
    return batch_processor

def main():
    """Main entry point for the video pipeline."""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Video Pipeline")
    parser.add_argument("--config", default="config.json", help="Path to configuration file")
    parser.add_argument("--log-dir", default="logs", help="Directory for log files")
    parser.add_argument("--log-level", default="INFO", help="Logging level")
    parser.add_argument("--source", default="pexels", help="Video source to use")
    parser.add_argument("--query", default="nature", help="Search query")
    parser.add_argument("--max-videos", type=int, default=10, help="Maximum number of videos to process")
    parser.add_argument("--batch-id", help="Resume processing for a specific batch ID")
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(args.log_dir, args.log_level)
    
    # Load configuration
    config = load_config(args.config)

    # Override storage provider from environment variable if present
    storage_provider = os.environ.get("STORAGE_PROVIDER")
    if storage_provider:
        config["storage"]["provider"] = storage_provider.lower()

    # Initialize pipeline
    batch_processor = initialize_pipeline(config)
    
    try:
        # Process batch or resume existing batch
        if args.batch_id:
            logging.info(f"Resuming batch {args.batch_id}")
            result = batch_processor.resume_batch(args.batch_id)
        else:
            logging.info(f"Processing new batch from {args.source} with query '{args.query}'")
            result = batch_processor.process_batch(args.source, args.query, args.max_videos)
        
        # Print result
        if result["success"]:
            logging.info(f"Batch processing completed successfully")
            logging.info(f"Videos found: {result['videos_found']}")
            logging.info(f"Videos downloaded: {result['videos_downloaded']}")
            logging.info(f"Videos validated: {result['videos_validated']}")
            logging.info(f"Videos uploaded: {result['videos_uploaded']}")
            logging.info(f"Videos failed: {result['videos_failed']}")
            logging.info(f"Duration: {result['duration']:.2f} seconds")
        else:
            logging.error(f"Batch processing failed: {result.get('error', 'Unknown error')}")
        
    except Exception as e:
        logging.error(f"Error in pipeline: {str(e)}")
    
    finally:
        # Clean up
        batch_processor.cleanup()

if __name__ == "__main__":
    main()

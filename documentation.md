# Video Pipeline Documentation

## Overview

This documentation provides a comprehensive guide to the enhanced video pipeline system. The pipeline is designed to scrape videos from multiple sources, validate them against specific criteria, and upload validated videos to cloud storage.

## Architecture

The pipeline consists of several modular components:

1. **Scrapers**: Modules for retrieving videos from various sources
2. **Validators**: Modules for validating video content against specific criteria
3. **Storage**: Module for uploading validated videos to cloud storage
4. **Batch Processor**: Module for orchestrating the entire pipeline process

## Scrapers

The following scrapers have been implemented:

- **Pexels Scraper**: Retrieves videos from Pexels API
- **Videvo Scraper**: Retrieves videos from Videvo API
- **NASA Scraper**: Retrieves videos from NASA Image and Video Library
- **Internet Archive Scraper**: Retrieves videos from Internet Archive
- **Wikimedia Scraper**: Retrieves videos from Wikimedia Commons
- **Coverr Scraper**: Retrieves videos from Coverr (web scraping)
- **NOAA Scraper**: Retrieves videos from NOAA

Each scraper adheres to its respective API documentation and rate limits.

## Validators

The following validators have been implemented:

- **Text Detection Validator**: Detects on-screen text using OCR (Tesseract)
- **Cut Scene Validator**: Detects scene changes using ffmpeg scene detection
- **Resolution Validator**: Ensures videos are at least 512×512 resolution
- **AI Content Validator**: Filters out AI-generated content based on metadata
- **Physics Realism Validator**: Verifies videos depict real-world physics

All validators are integrated into a unified validation pipeline.

## Storage

The cloud storage module supports:

- AWS S3
- Google Cloud Storage
- Azure Blob Storage

It handles:
- Uploading validated videos
- Tracking upload history
- Retry logic for failed uploads
- Deduplication

## Batch Processor

The batch processor orchestrates the entire pipeline:

1. Searches for videos using specified scraper
2. Downloads videos in batches
3. Validates videos using the validation pipeline
4. Uploads validated videos to cloud storage
5. Manages disk space by cleaning up processed videos
6. Tracks batch state for resuming interrupted processes

## Usage

### Configuration

Create a `config.json` file with the following structure:

```json
{
  "scrapers": {
    "pexels": {"per_page": 20},
    "videvo": {"per_page": 20, "request_delay": 1.0},
    "nasa": {"per_page": 20, "request_delay": 3.6},
    "internet_archive": {"per_page": 20, "request_delay": 1.0},
    "wikimedia": {"per_page": 20, "request_delay": 1.0},
    "coverr": {"per_page": 20, "request_delay": 3.0},
    "noaa": {"per_page": 20, "request_delay": 2.0}
  },
  "validators": {
    "text_detection": {
      "sampling_rate": 30,
      "confidence_threshold": 70,
      "min_text_detections": 3
    },
    "cut_scene": {
      "threshold": 0.35,
      "min_scene_changes": 2,
      "frame_skip": 1
    },
    "resolution": {
      "min_width": 512,
      "min_height": 512
    },
    "ai_content": {
      "ai_keywords": ["ai generated", "artificial intelligence", "generated by ai"]
    },
    "physics": {
      "sampling_rate": 15,
      "optical_flow_threshold": 50.0,
      "acceleration_threshold": 100.0,
      "min_violations": 3
    },
    "log_file": "logs/validation.log",
    "detailed_logs": true
  },
  "storage": {
    "provider": "aws",
    "bucket_name": "video-pipeline-bucket",
    "folder_prefix": "videos/",
    "region": "us-east-1",
    "upload_history_file": "logs/upload_history.json",
    "max_retries": 3,
    "retry_delay": 2
  },
  "batch": {
    "download_dir": "downloads",
    "processed_dir": "processed",
    "failed_dir": "failed",
    "batch_size": 10,
    "max_workers": 4,
    "disk_space_threshold": 1073741824,
    "state_file": "logs/batch_state.json"
  }
}
```

### Environment Variables

Set the following environment variables:

```
PEXELS_API_KEY=your_pexels_api_key
VIDEVO_API_KEY=your_videvo_api_key
NASA_API_KEY=your_nasa_api_key
IA_ACCESS_KEY=your_internet_archive_access_key
IA_SECRET_KEY=your_internet_archive_secret_key
NOAA_API_TOKEN=your_noaa_api_token
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
```

### Running the Pipeline

Run the pipeline using the following command:

```bash
python main.py --source pexels --query nature --max-videos 10
```

Options:
- `--config`: Path to configuration file (default: `config.json`)
- `--log-dir`: Directory for log files (default: `logs`)
- `--log-level`: Logging level (default: `INFO`)
- `--source`: Video source to use (default: `pexels`)
- `--query`: Search query (default: `nature`)
- `--max-videos`: Maximum number of videos to process (default: `10`)
- `--batch-id`: Resume processing for a specific batch ID

## Dependencies

- Python 3.8+
- OpenCV
- FFmpeg
- Tesseract OCR
- Boto3 (for AWS S3)
- Google Cloud Storage (for GCP)
- Azure Blob Storage (for Azure)
- NumPy
- Requests

## Testing

Comprehensive tests are included for all components:

- Unit tests for individual modules
- Integration tests for component interactions
- End-to-end tests for the full pipeline

Run tests using:

```bash
python -m unittest discover tests
```

## Implementation Notes

1. The resolution requirement has been updated to ensure videos are at least 512×512 resolution, rather than resizing them to exactly 512×512.

2. All scrapers adhere to their respective API documentation and rate limits.

3. The batch processor manages disk space by cleaning up processed videos.

4. The validation pipeline is modular and can be extended with additional validators.

5. The cloud storage module supports multiple cloud providers and includes retry logic for reliability.

6. The system is designed to be robust against failures and can resume interrupted processes.
